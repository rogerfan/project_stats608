\documentclass{article}

\usepackage{nips15submit_e,times}
\nipsfinalcopy % Uncomment for camera-ready version

% General
\usepackage[numbers]{natbib}
\usepackage{setspace}
\usepackage{geometry}
\usepackage[section]{placeins}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage{color}
\usepackage{titlesec}
\usepackage[page]{appendix}
\usepackage{enumerate}

% Tables/Figures
\usepackage{lscape}
\usepackage{booktabs}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

% Math
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{dsfont}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% User-defined LaTeX commands
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand*{\expp}[1]{\exp\left(#1\right)}
\newcommand*{\foralls}{\ \forall \ }
\newcommand*{\st}{\text{ s.t. }}
\newcommand*{\E}{\mathbb E}
\newcommand*{\R}{\mathbb R}
\newcommand*{\I}{\mathds{1}}
\newcommand*{\Prob}{\mathbb P}
\newcommand*{\convas}[1]{\xrightarrow{#1}}
\newcommand*{\conv}{\convas{}}
\newcommand*{\cond}{\;\ifnum\currentgrouptype=16 \middle\fi|\;}
\newcommand*{\defeq}{%
  \mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}
\newcommand*{\notorth}{\ensuremath{\perp\!\!\!\!\!\!\diagup\!\!\!\!\!\!\perp}}
\newcommand*{\orth}{\ensuremath{\perp\!\!\!\perp}}
\newcommand*{\evalat}{\,\big\rvert}
\newcommand*{\dif}{\,\mathrm{d}}
\newcommand*{\difto}[1]{\,\mathrm{d^#1}}
\newcommand*{\difbot}[1]{\frac{\mathrm{d}}{\mathrm{d}#1}}
\newcommand*{\partialbot}[1]{\frac{\partial}{\partial#1}}
\newcommand*{\m}[1]{\textbf{#1}}
\newcommand*{\bmath}[1]{\boldsymbol{#1}}

\newcommand*{\yestag}{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand*{\notaligned}[1]{\noalign{$\displaystyle #1$}}
\newcommand*{\ttilde}{{\raise.17ex\hbox{$\scriptstyle\sim$}}}

\makeatletter
\newsavebox{\mybox}\newsavebox{\mysim}
\newcommand*{\distas}[1]{%
  \savebox{\mybox}{\hbox{\kern3pt$\scriptstyle#1$\kern3pt}}%
  \savebox{\mysim}{\hbox{$\sim$}}%
  \mathbin{\overset{#1}{\kern\z@\resizebox{\wd\mybox}{\ht\mysim}{$\sim$}}}%
}
\makeatother
\newcommand*{\dist}{\sim}
\newcommand*{\distiid}{\distas{\text{i.i.d}}}

\makeatletter
\def\moverlay{\mathpalette\mov@rlay}
\def\mov@rlay#1#2{\leavevmode\vtop{%
   \baselineskip\z@skip \lineskiplimit-\maxdimen
   \ialign{\hfil$\m@th#1##$\hfil\cr#2\crcr}}}
\newcommand*{\charfusion}[3][\mathord]{
  #1{\ifx#1\mathop\vphantom{#2}\fi\mathpalette\mov@rlay{#2\cr#3}}
  \ifx#1\mathop\expandafter\displaylimits\fi}
\makeatother
\newcommand*{\cupdot}{\charfusion[\mathbin]{\cup}{\cdot}}
\newcommand*{\bigcupdot}{\charfusion[\mathop]{\bigcup}{\cdot}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem*{properties}{Properties}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Stats 608A: Project Proposal}
\author{
  Joseph Dickens \\
  University of Michigan \\
  \texttt{josephdi@umich.edu} \\
\And
  Roger Fan \\
  University of Michigan\\
  \texttt{rogerfan@umich.edu} \\
}

\maketitle


\section{Problem Statement}

Mixture models are a common and powerful method for clustering and distribution estimation. Data is modeled as a ``mixture'' of multiple simple distributions, most often Gaussian, where the goal is to estimate the parameters of each of the distributions. The distribution that each data point belongs to is generally thought of as a latent unobserved variable. Because the possible values of this latent variable can be extremely large, direct maximization of the likelihood is generally difficult.

The most common method used to estimate parameters in this setting is the Expectation-Maximization (EM) algorithm \cite{dempsterlairdrubin77}. The EM algorithm is an iterative optimization method used to optimize a lower bound on the likelihood. There are, however, several issues with using the EM algorithm even in this fairly simple case. Mixture models often have multiple local optima. Since EM is a hill-climbing algorithm, it may converge to a local optimum rather than a global optima \cite{wu83}. This makes EM sensitive to the chosen starting point, and various ad-hoc methods (such as multiple start or random restart) are used in practice to attempt to mitigate this issue. It can also be slow to converge for a variety of reasons, including when the mixing coefficients are imbalanced or when there is significant overlap in the component distributions.


\section{Related Work}

Previous attempts to improve upon the EM algorithm can be found in the literature. Of particular interest is simulated annealing \cite{kirkpatrickgelattvecchi83}, a probabilistic optimization algorithm designed to find global optima. Methods similar to \cite{kirkpatrickgelattvecchi83} mimic EM, but replace the traditional ``E-step'' with a computationally attractive alternative. Such methods seek to explore the state space in a non-deterministic fashion. The non-deterministic nature of such methods makes multiple initializations unnecessary and hopefully improves the optimization by avoiding convergence to local optima.

There have been several attempts to apply annealing-inspired algorithms to mixture model estimation. The most notable algorithm is Deterministic Annealing EM (DAEM), proposed by Ueda and Nakano in 1998 \cite{uedanakano98}. As the name suggests, this algorithm replaces the stochastic nature of simulated annealing with a deterministic variant to hopefully avoid the local optima problem. Naim and Gildea proposed another deterministic algorithm very similar to DAEM that they call anti-annealing in 2012 \cite{naimgildea12} and also compare the performance of several optimization algorithms, including DAEM and conventional EM, on a mixture density estimation problem.


\section{Proposed Work}

% potential work for us
Similar to the works listed in section 2, we seek to create an algorithm that borrows attractive properties from both EM and non-deterministic methods.
Because calculating the likelihood conditional on a set of state variable realizations is extremely fast and easy, we can potentially re-parameterize the problem as finding the state realization that induces the highest conditional likelihood and apply simulated annealing or similar optimizers to this problem.

We specifically hope to improve performance by limiting non-deterministic methods' propensity to explore ``bad'' latent states. If we can identify observations which almost certainly belong to a single latent state, we hope to avoid exploring spaces in which these observations belong to other states. If these ``bad'' states are relatively rare or already unattractive to the methods, it is certainly feasible that avoiding such states will have little or perhaps a negative impact on performance.


\section{Evaluation Strategy}

Although several deterministic annealing algorithms have been developed for these problems, to our knowledge the performance of stochastic algorithms closer to traditional simulated annealing has not been well explored. We hope to develop and implement a simulated annealing algorithm for mixture distribution estimation and compare its performance to EM and other existing algorithms.

It is also important to note that there are two important criteria for evaluating these algorithms, as we care both about time taken/convergence speed and ability to avoid local optima. EM generally has fairly good performance in terms of convergence speed for many problems, but it may be that other algorithms are better at identifying global optima despite slower convergence speed.

If time and space permit, performance on real data will also hopefully be assessed. Mixture models can be and are commonly applied to a vast array of clustering problems. It is also possible that these annealing-based methods could be extended to related problems that EM is often used for, such as the estimation of Hidden Markov Models or other models with unobserved latent variables.


% \vfill
% \FloatBarrier \pagebreak
% \nocite{*}
\bibliographystyle{unsrt}
\bibliography{biblio}


\end{document}
