\documentclass{article}

\usepackage{nips15submit_e,times}
\nipsfinalcopy % Uncomment for camera-ready version

% General
\usepackage[numbers]{natbib}
\usepackage{setspace}
\usepackage{geometry}
\usepackage[section]{placeins}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage{color}
\usepackage{titlesec}
\usepackage[page]{appendix}
\usepackage{enumerate}

% Tables/Figures
\usepackage{lscape}
\usepackage{booktabs}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

% Math
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{dsfont}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% User-defined LaTeX commands
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand*{\expp}[1]{\exp\left(#1\right)}
\newcommand*{\foralls}{\ \forall \ }
\newcommand*{\st}{\text{ s.t. }}
\newcommand*{\E}{\mathbb E}
\newcommand*{\R}{\mathbb R}
\newcommand*{\I}{\mathds{1}}
\newcommand*{\Prob}{\mathbb P}
\newcommand*{\convas}[1]{\xrightarrow{#1}}
\newcommand*{\conv}{\convas{}}
\newcommand*{\cond}{\;\ifnum\currentgrouptype=16 \middle\fi|\;}
\newcommand*{\defeq}{%
  \mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}
\newcommand*{\notorth}{\ensuremath{\perp\!\!\!\!\!\!\diagup\!\!\!\!\!\!\perp}}
\newcommand*{\orth}{\ensuremath{\perp\!\!\!\perp}}
\newcommand*{\evalat}{\,\big\rvert}
\newcommand*{\dif}{\,\mathrm{d}}
\newcommand*{\difto}[1]{\,\mathrm{d^#1}}
\newcommand*{\difbot}[1]{\frac{\mathrm{d}}{\mathrm{d}#1}}
\newcommand*{\partialbot}[1]{\frac{\partial}{\partial#1}}
\newcommand*{\m}[1]{\textbf{#1}}
\newcommand*{\bmath}[1]{\boldsymbol{#1}}

\newcommand*{\yestag}{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand*{\notaligned}[1]{\noalign{$\displaystyle #1$}}
\newcommand*{\ttilde}{{\raise.17ex\hbox{$\scriptstyle\sim$}}}

\makeatletter
\newsavebox{\mybox}\newsavebox{\mysim}
\newcommand*{\distas}[1]{%
  \savebox{\mybox}{\hbox{\kern3pt$\scriptstyle#1$\kern3pt}}%
  \savebox{\mysim}{\hbox{$\sim$}}%
  \mathbin{\overset{#1}{\kern\z@\resizebox{\wd\mybox}{\ht\mysim}{$\sim$}}}%
}
\makeatother
\newcommand*{\dist}{\sim}
\newcommand*{\distiid}{\distas{\text{i.i.d}}}

\makeatletter
\def\moverlay{\mathpalette\mov@rlay}
\def\mov@rlay#1#2{\leavevmode\vtop{%
   \baselineskip\z@skip \lineskiplimit-\maxdimen
   \ialign{\hfil$\m@th#1##$\hfil\cr#2\crcr}}}
\newcommand*{\charfusion}[3][\mathord]{
  #1{\ifx#1\mathop\vphantom{#2}\fi\mathpalette\mov@rlay{#2\cr#3}}
  \ifx#1\mathop\expandafter\displaylimits\fi}
\makeatother
\newcommand*{\cupdot}{\charfusion[\mathbin]{\cup}{\cdot}}
\newcommand*{\bigcupdot}{\charfusion[\mathop]{\bigcup}{\cdot}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem*{properties}{Properties}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\title{Stats 608A: Project Proposal}
\author{
  Joseph Dickens \\
  University of Michigan \\
  \texttt{josephdi@umich.edu} \\
\And
  Roger Fan \\
  University of Michigan\\
  \texttt{rogerfan@umich.edu} \\
}

\maketitle


Mixture models are a common and powerful method for clustering and distribution estimation. Data is modeled as a ``mixture'' of multiple simple distributions, most often Gaussian, where the goal is to estimate the parameters of each of the distributions. The distribution that each data point belongs to is generally thought of as a latent unobserved variable. Because the possible values of this latent variable can be extremely large, direct maximization of the likelihood is generally difficult.

The most common method used to estimate parameters in this setting is the Expectation-Maximization (EM) algorithm \cite{dempsterlairdrubin77}. The EM algorithm is an iterative optimization method used to optimize a lower bound on the likelihood. There are, however, several issues with using the EM algorithm even in this fairly simple case. Mixture models often have multiple local optima, and EM is a hill-climbing algorithm that can only find local maxima \cite{wu83}. This makes EM sensitive to the chosen starting point, and various ad-hoc methods (such as multiple start or random restart) are used in practice to attempt to mitigate this issue. It can also be slow to converge for a variety of reasons, including when the mixing coefficients are imbalanced or when there is significant overlap in the component distributions.

We plan to explore some alterative optimization algorithms to this problem and compare their performance to EM using simulated and real data. Of particular interest is simulated annealing \cite{kirkpatrickgelattvecchi83}, a probabilistic optimization algorithm designed to find global optima. Because calculating the likelihood conditional on a set of state variable realizations is extremely fast and easy, we can potentially reparameterize the problem as finding the state realization that induces the highest conditional likelihood and apply simulated annealing or similar optimizers to this problem. These estimators will likely look very similar to EM, except replacing the traditional ``E-step'' with an alternate method of exploring the state space.

There have been several attempts to apply annealing-inspired algorithms to mixture model estimation. The most notable algorithm is Deterministic Annealing EM (DAEM), proposed by Ueda and Nakano in 1998 \cite{uedanakano98}. As the name suggests, this algorithm does not keep the stochastic nature of simulated annealing, but uses a deterministic variant to hopefully avoid the local optima problem. Naim and Gildea proposed another deterministic algorithm very similar to DAEM that they call anti-annealing in 2012 \cite{naimgildea12} and also compare the performance of several optimization algorithms, including DAEM and conventional EM, on a mixture density estimation problem.

Although several deterministic annealing algorithms have been developed for these problems, to our knowledge the performance of stochastic algorithms closer to traditional simulated annealing has not been well explored. We hope to develop and implement a simulated annealing algorithm for mixture distribution estimation and compare its performance to EM and other existing algorithms.

It is also important to note that there are two important criteria for evaluating these algorithms, as we care both about time taken/convergence speed and ability to avoid local optima. EM generally has fairly good performance in terms of convergence speed for many problems, but it may be that other algorithms are better at identifying global optima despite slower convergence speed.

If time and space permit, performance on real data will also hopefully be assessed. Mixture models can be and are commonly applied to a vast array of clustering problems. It is also possible that these annealing-based methods could be extended to related problems that EM is often used for, such as the estimation of Hidden Markov Models or other models with unobserved latent variables.





% \vfill
% \FloatBarrier \pagebreak
% \nocite{*}
\bibliographystyle{unsrt}
\bibliography{biblio}


\end{document}
